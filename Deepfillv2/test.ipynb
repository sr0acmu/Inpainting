{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import torchvision as tv\n",
                "import torchvision.transforms as T\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "\n",
                "from model.networks_tf import Generator, Discriminator\n",
                "#from model.networks import Generator, Discriminator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load generator model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "use_cuda_if_available = True\n",
                "device = torch.device('cuda' if torch.cuda.is_available() \n",
                "                             and use_cuda_if_available else 'cpu')\n",
                "generator = Generator(cnum_in=5, cnum=48, return_flow=True).to(device)\n",
                "\n",
                "generator_state_dict = torch.load('pretrained/states_tf_places2.pth')['G']\n",
                "generator.load_state_dict(generator_state_dict)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load image and mask\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n = 3\n",
                "image = f\"examples/inpaint/case{n}.png\"\n",
                "mask = f\"examples/inpaint/case{n}_mask.png\"\n",
                "\n",
                "image = Image.open(image)\n",
                "mask = Image.open(mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plot raw image and mask\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15, 10))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(image)  # plot raw image\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(mask)   # plot masked image\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocess image and mask\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# preprocessing\n",
                "image_org = T.ToTensor()(image)\n",
                "mask = T.ToTensor()(mask)\n",
                "\n",
                "_, h, w = image_org.shape\n",
                "grid = 8\n",
                "\n",
                "image = image_org[:3, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
                "mask = mask[0:1, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
                "\n",
                "print(f\"Shape of image: {image.shape}\")\n",
                "\n",
                "image = (image*2 - 1.).to(device)  # map image values to [-1, 1] range\n",
                "mask = (mask > 0.).to(dtype=torch.float32, device=device)  # 1.: masked 0.: unmasked\n",
                "\n",
                "image_masked = image * (1.-mask)  # mask image\n",
                "\n",
                "ones_x = torch.ones_like(image_masked)[:, 0:1, :, :]\n",
                "x = torch.cat([image_masked, ones_x, ones_x*mask], dim=1)  # concatenate channels\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inpaint masked image\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with torch.no_grad():\n",
                "    x_stage1, x_stage2, offset_flow = generator(x, mask)\n",
                "\n",
                "image_inpainted = image_masked * (1.-mask) + x_stage2 * mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pt_to_rgb(pt): return pt[0].cpu().permute(1, 2, 0)*0.5 + 0.5\n",
                "\n",
                "print(\"Result:\")\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.imshow(pt_to_rgb(image_inpainted))\n",
                "plt.show()\n",
                "\n",
                "print(\"| Raw | Masked |\\n| Stage1 | Stage2 |\")\n",
                "plt.figure(figsize=(15, 10))\n",
                "plt.subplot(2, 2, 1)\n",
                "plt.imshow(pt_to_rgb(image))\n",
                "plt.subplot(2, 2, 2)\n",
                "plt.imshow(pt_to_rgb(image_masked))\n",
                "plt.subplot(2, 2, 3)\n",
                "plt.imshow(pt_to_rgb(x_stage1))\n",
                "plt.subplot(2, 2, 4)\n",
                "plt.imshow(pt_to_rgb(x_stage2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plot attention flow map\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(offset_flow[0].cpu().permute(1, 2, 0))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Contextual Attention\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from model.networks import ContextualAttention\n",
                "#from model.networks_tf import ContextualAttention\n",
                "\n",
                "\n",
                "contextual_attention = ContextualAttention(ksize=3, stride=1, rate=2, \n",
                "                                           fuse_k=3, softmax_scale=10, \n",
                "                                           fuse=False,\n",
                "                                           return_flow=True)\n",
                "\n",
                "imageB = 'examples/style_transfer/bike.jpg'\n",
                "imageA = 'examples/style_transfer/bnw_butterfly.png'\n",
                "\n",
                "def test_contextual_attention(imageA, imageB):\n",
                "    \"\"\"Test contextual attention layer with 3-channel image input\n",
                "    (instead of n-channel feature).\n",
                "    \n",
                "    \"\"\"\n",
                "    rate = 2\n",
                "    stride = 1\n",
                "    grid = rate*stride\n",
                "    \n",
                "    b = Image.open(imageA)\n",
                "    b = b.resize((b.width//2, b.height//2), resample=Image.BICUBIC)\n",
                "    b = T.ToTensor()(b)\n",
                "\n",
                "    _, h, w = b.shape\n",
                "    b = b[:, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
                "\n",
                "    print(f\"Size of imageA: {b.shape}\")\n",
                "\n",
                "    f = T.ToTensor()(Image.open(imageB)) \n",
                "    _, h, w = f.shape\n",
                "    f = f[:, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
                " \n",
                "    print(f\"Size of imageB: {f.shape}\")   \n",
                "\n",
                "    yt, flow = contextual_attention(f*255., b*255.)\n",
                "\n",
                "    return yt, flow\n",
                "\n",
                "\n",
                "yt, flow = test_contextual_attention(imageA, imageB)\n",
                "\n",
                "plt.figure(figsize=(15, 10))\n",
                "plt.subplot(2, 2, 1)\n",
                "plt.imshow(yt[0].permute(1, 2, 0)/255.)\n",
                "plt.subplot(2, 2, 2)\n",
                "plt.imshow(flow[0].permute(1, 2, 0))\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
